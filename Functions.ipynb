{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6ce4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To run the functions in another notebook file, use the following:\\\n",
    "# %run Functions.ipynb     #this is were my function was stored\\\n",
    "# function(df)                    #then simply run the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458feff",
   "metadata": {},
   "source": [
    "# Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b33ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainTestSplit(df,separate = 1,latest_year = 2017):\n",
    "    print('Train test split')\n",
    "    clientID = df.ClientID.unique()\n",
    "    if separate == 1:\n",
    "        # separating the living situation and the exit destination into categories\n",
    "        goal = [10,11]\n",
    "        closer = [3,19,20,21,22,23,25,26,28,31]\n",
    "        trans = [1,2,4,5,6,12,13,14,15,18,27,29]\n",
    "        no_progress = [7,16]\n",
    "        hard_to_judge = [8,9,17,24,30,99]\n",
    "        # Separating out a sample of data\n",
    "        sample_data = pd.DataFrame()\n",
    "        for i in range(len(clientID)):\n",
    "            temp = df[df['ClientID']==clientID[i]]\n",
    "            sample_data = sample_data.append(temp)\n",
    "        sample_data = sample_data.reset_index(drop=True)\n",
    "        # sample_data.to_csv(rpath+\"//20220329_Sample data of 10 clients.csv\")\n",
    "\n",
    "        # Separating out the data with ultimate goal, closer to exit, and transitional phase\n",
    "        sample_client = sample_data.ClientID.unique()\n",
    "        data = pd.DataFrame()\n",
    "        for i in range(len(sample_client)):\n",
    "            temp = sample_data[sample_data['ClientID']==sample_client[i]].reset_index(drop=True)\n",
    "            if temp.Destination.iloc[-1] in goal or temp.Destination.iloc[-1] in closer or temp.Destination.iloc[-1] in trans:\n",
    "                data = data.append(temp)\n",
    "    else:\n",
    "        data = df\n",
    "    \n",
    "    \n",
    "    #Determining train data\n",
    "    print(\"Preparing train data\")\n",
    "    client = data.ClientID.unique()\n",
    "    trainData = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(client)):\n",
    "        temp = data[data['ClientID']==client[i]].reset_index(drop=True)\n",
    "        temp = temp[temp['EntryDate'].dt.year<latest_year]\n",
    "        if len(temp)==0:\n",
    "            continue\n",
    "        trainData = trainData.append(temp)\n",
    "\n",
    "\n",
    "    # Determining historical data and queries\n",
    "    print(\"Preparing test data\")\n",
    "    testData = pd.DataFrame()\n",
    "    for i in range(len(client)):\n",
    "        temp = data[data['ClientID']==client[i]].reset_index(drop=True)\n",
    "        temp = temp[temp['EntryDate'].dt.year>=latest_year]\n",
    "        if len(temp)==0:\n",
    "            continue\n",
    "        testData = testData.append(temp)\n",
    "\n",
    "\n",
    "    # historical data: 80% of TestData\n",
    "    # queries: 20% of TestData\n",
    "\n",
    "    test_client_list = testData.ClientID.unique()\n",
    "\n",
    "    historicalPercent = 0.8\n",
    "    queryPercent = 1 - historicalPercent\n",
    "    print(\"Preparing historical and query data\")\n",
    "    historicalClient = pd.DataFrame(test_client_list[0:int(historicalPercent*len(test_client_list))])\n",
    "    queryClient = pd.DataFrame(test_client_list[int(historicalPercent*len(test_client_list))::])\n",
    "\n",
    "    \n",
    "    return trainData, testData, historicalClient, queryClient\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6eeeb",
   "metadata": {},
   "source": [
    "# Transition graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bdd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_graph_snapshot(trainData):\n",
    "    print('Preparing snapshot graph')\n",
    "    # Categorizing the training data using the effective length\n",
    "    trainClient = trainData.ClientID.unique()\n",
    "    trainData_lengthSeparated = pd.DataFrame()\n",
    "    for i in range(len(trainClient)):\n",
    "        temp = trainData[trainData['ClientID']==trainClient[i]].reset_index(drop=True)\n",
    "        effective_length = len(temp['ProjectType'].unique())\n",
    "        temp['effective length'] = [effective_length for j in range(len(temp))]\n",
    "        trainData_lengthSeparated = trainData_lengthSeparated.append(temp)\n",
    "    #Building transition graph for each effective length\n",
    "    effective_length_unique = trainData_lengthSeparated['effective length'].unique()\n",
    "    normalized_tm_combined = pd.DataFrame()\n",
    "    for n in range(len(effective_length_unique)):\n",
    "        print(effective_length_unique[n])\n",
    "        temp1 = trainData_lengthSeparated[trainData_lengthSeparated['effective length']==effective_length_unique[n]]\n",
    "        temp_clientID = temp1['ClientID'].unique()\n",
    "\n",
    "        # Using dynamic programming style to determine the edges at each step considering the hops\n",
    "        DP_edge_df = pd.DataFrame()\n",
    "        for i in range(len(temp_clientID)):\n",
    "            temp = temp1[temp1['ClientID']==temp_clientID[i]]\n",
    "            project_list = list(temp.ProjectType)+[-3]\n",
    "        #     print(project_list)\n",
    "            DP = pd.DataFrame(index=range(len(project_list)),columns = [k for k in range(1,len(project_list)+1)])\n",
    "            closed = []\n",
    "            for d in range(1,len(project_list)+1):\n",
    "                for a in range(len(project_list)):\n",
    "                    for b in range(len(project_list)):\n",
    "                        if (project_list[a],project_list[b],d) not in closed:\n",
    "                            if (b-a) == d:\n",
    "                                DP.at[a,d] = (project_list[a],project_list[b])\n",
    "                                DP.at[a,'Position'] = a+1\n",
    "                                closed = closed + [(project_list[a],project_list[b],d)]\n",
    "            DP = DP.dropna(how = 'all',axis = 0)\n",
    "            DP_edge_df = DP_edge_df.append(DP)\n",
    "\n",
    "        DP_edge_df = DP_edge_df.dropna(how = 'all',axis = 1)  \n",
    "\n",
    "        # Determining unique edges and creating a matrix for each edge\n",
    "        edge_list = []\n",
    "        for i in DP_edge_df.columns:\n",
    "            if i!='Position':\n",
    "                temp = DP_edge_df[i].dropna().unique()\n",
    "                edge_list = edge_list + list(temp)\n",
    "\n",
    "        edge_list_df = pd.DataFrame()\n",
    "        edge_list_df['edges'] = [edge_list[i] for i in range(len(edge_list))]\n",
    "        edge_list_unique = edge_list_df['edges'].unique()\n",
    "        hop_unique = DP_edge_df['Position'].unique()\n",
    "\n",
    "        # Counting the edges in each step\n",
    "        edge_count = pd.DataFrame()\n",
    "        for k in hop_unique:\n",
    "            for i in edge_list_unique:\n",
    "                temp = pd.DataFrame(index = edge_list_unique, columns = list(DP_edge_df.columns))\n",
    "                for j in DP_edge_df.columns:\n",
    "                    if j!='Position':\n",
    "                        temp.at[i,j] = len(DP_edge_df[(DP_edge_df[j]==i) & (DP_edge_df['Position']==k)])            \n",
    "                temp.at[i,'Position'] = k \n",
    "                temp = temp.dropna()\n",
    "                edge_count = edge_count.append(temp)\n",
    "\n",
    "        edge_count = edge_count.reset_index()\n",
    "        edge_count = edge_count.rename(columns = {'index':'edges'})\n",
    "\n",
    "        edge_count_matrix = edge_count.groupby(['edges','Position']).sum()\n",
    "\n",
    "        # adding weights based on the hop and position\n",
    "        alpha = 0.5\n",
    "        weighted_edge_matrix = pd.DataFrame()\n",
    "        for i in edge_list_unique:\n",
    "            temp = edge_count_matrix.loc[i]\n",
    "            weighted_edge = pd.DataFrame(index = temp.index,columns = temp.columns)\n",
    "            for j in range(len(temp.columns)):#Hops\n",
    "                for k in range(len(temp.index)):#Position\n",
    "                    weighted_edge.at[temp.index[k],temp.columns[j]] = (alpha**(j+k))  * (temp.at[temp.index[k],temp.columns[j]])\n",
    "        #             if weighted_edge.at[temp.index[k],temp.columns[j]]:\n",
    "        #                 print(j,k)\n",
    "        #                 print(weighted_edge.at[temp.index[k],temp.columns[j]],(alpha**(j+k)),(temp.at[temp.index[k],temp.columns[j]]))\n",
    "            weighted_edge['edges'] = [i for a in range(len(weighted_edge))]\n",
    "            weighted_edge_matrix = weighted_edge_matrix.append(weighted_edge)\n",
    "\n",
    "        # summing the elements of the matrix to get the total weight\n",
    "        final_weights = pd.DataFrame(index=edge_list_unique,columns=['weight'])\n",
    "        for i in edge_list_unique:\n",
    "            final_weights.at[i,'weight'] = weighted_edge_matrix[weighted_edge_matrix['edges']==i].drop('edges',axis=1).sum().sum()    \n",
    "\n",
    "\n",
    "\n",
    "        # summing the elements of the matrix to get the total number of times the edges appear in the data\n",
    "        final_weights['total'] = np.nan\n",
    "        for i in edge_list_unique:\n",
    "            final_weights.at[i,'total'] = int(edge_count_matrix.loc[i].sum().sum())   \n",
    "\n",
    "        final_weights['normalized weight'] = final_weights['weight']/final_weights['total']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Building the transition matrix\n",
    "        transition_matrix = pd.DataFrame(index = list(np.sort(trainData['ProjectType'].unique()))+[-3], columns = list(np.sort(trainData['ProjectType'].unique()))+[-3])\n",
    "        for i in final_weights.index:\n",
    "            transition_matrix.at[i[0],i[1]] = final_weights.at[i,'normalized weight']\n",
    "\n",
    "        # Normalizing the transition matrix\n",
    "        normalized_tm = pd.DataFrame()\n",
    "        for i in transition_matrix.index:\n",
    "            temp = transition_matrix.loc[[i]]\n",
    "            denum = temp.sum(axis=1)[i]\n",
    "            if temp.isnull().all(axis=1).loc[i]:\n",
    "                normalized_tm = normalized_tm.append(temp)\n",
    "                continue\n",
    "            temp = temp/denum\n",
    "            normalized_tm = normalized_tm.append(temp)\n",
    "\n",
    "        normalized_tm['effective length'] = [effective_length_unique[n] for k in range(len(normalized_tm))]\n",
    "        normalized_tm = normalized_tm.reset_index()\n",
    "        normalized_tm_combined = normalized_tm_combined.append(normalized_tm)\n",
    "\n",
    "    return normalized_tm_combined\n",
    "\n",
    "\n",
    "def transition_graph_aggregate(trainData):\n",
    "    print('Preparing aggregate graph ')\n",
    "    # Generating the transition graph for the project type of train data\n",
    "    # Unique Edge list\n",
    "    edge_list = []\n",
    "    trainClient = trainData.ClientID.unique()\n",
    "    for i in range(len(trainClient)):\n",
    "        temp = trainData[trainData['ClientID']==trainClient[i]]\n",
    "        trajectory = temp.ProjectType.values\n",
    "        for j in range(len(trajectory)):\n",
    "            if j == len(trajectory)-1:\n",
    "                edge_list = edge_list + [(trajectory[j],-3)] \n",
    "                break\n",
    "            edge_list = edge_list + [(trajectory[j],trajectory[j+1])]\n",
    "\n",
    "    edge_list = pd.DataFrame({'edge_list':edge_list})\n",
    "    edge_unique = edge_list['edge_list'].unique()\n",
    "\n",
    "    # Calculating the number of times a node appear in the outgoing end of the edge\n",
    "    graph_nodes = pd.Series([edge_unique[i][0] for i in range(len(edge_unique))])\n",
    "    graph_nodes_count = pd.DataFrame(index = graph_nodes.unique(),columns = ['count'])\n",
    "    nodeOfOutgoingEdge = pd.Series([edge_list['edge_list'][i][0] for i in range(len(edge_list))])\n",
    "    for i in range(len(graph_nodes_count)):\n",
    "        n = len(nodeOfOutgoingEdge[nodeOfOutgoingEdge==graph_nodes_count.index[i]])\n",
    "        graph_nodes_count.at[graph_nodes_count.index[i],'count'] = n\n",
    "\n",
    "        # Calculating the transition probability of the edges in the data\n",
    "    edge_probability = pd.DataFrame(columns = ['probability'],index=edge_unique)\n",
    "    for i in range(len(edge_unique)):\n",
    "        n = len(edge_list[edge_list['edge_list']==edge_unique[i]])\n",
    "        outgoing_node_count = graph_nodes_count.loc[edge_unique[i][0]]['count']\n",
    "        edge_probability.at[edge_unique[i],'probability'] = n/outgoing_node_count\n",
    "\n",
    "    # Generating the transition matrix (rows:starting point of an edge, columns:ending point of an edge)\n",
    "    # sorted unique project types in the edges\n",
    "    ind = list(np.sort(trainData['ProjectType'].unique()))+[-3]\n",
    "    col = list(np.sort(trainData['ProjectType'].unique()))+[-3]\n",
    "    transition_matrix = pd.DataFrame(columns = col,index=ind)\n",
    "    for i in range(len(edge_probability)):\n",
    "        edge = edge_probability.index[i]\n",
    "        transition_matrix.at[edge[0],edge[1]] = edge_probability.iloc[i].values[0]\n",
    "    transition_matrix = transition_matrix.reset_index()    \n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def GraphConstruction(transition_matrix,method):\n",
    "    print('Constructing the graph in networkx')\n",
    "    if method == 'snapshot':\n",
    "        # Developing the transition graphs for each effective length in networkx\n",
    "        normalized_tm_combined = transition_matrix\n",
    "        effective_length_unique = normalized_tm_combined['effective length'].unique()\n",
    "        projectType = normalized_tm_combined['index'].unique()\n",
    "\n",
    "        G_list = {}\n",
    "        Dg_list = {}\n",
    "        shortest_path_length_list = {}\n",
    "        for n in range(len(effective_length_unique)):\n",
    "            print(n)\n",
    "            G = nx.DiGraph()\n",
    "            none = [G.add_node(k) for k in projectType]\n",
    "            normalized_tm = normalized_tm_combined[normalized_tm_combined['effective length']==effective_length_unique[n]]\n",
    "            normalized_tm = normalized_tm.set_index('index')\n",
    "            normalized_tm = normalized_tm.drop('effective length',axis=1)\n",
    "            normalized_tm.columns = pd.to_numeric(normalized_tm.columns)\n",
    "            for i in range(len(normalized_tm.index)):\n",
    "                nodes = normalized_tm.loc[normalized_tm.index[i]][normalized_tm.loc[normalized_tm.index[i]].notna()].index.values\n",
    "                for k in nodes:\n",
    "                    G.add_edge(normalized_tm.index[i],k,weight=normalized_tm.loc[normalized_tm.index[i]][k])  \n",
    "#             plt.figure(figsize=(5,5))\n",
    "            pos = nx.shell_layout(G)\n",
    "            degree = dict(G.degree)\n",
    "            edges = G.edges()\n",
    "            weights = [G[u][v]['weight'] for u,v in edges]\n",
    "#             nx.draw(G, pos, with_labels = True,node_size=[v * 100 for v in degree.values()], node_color=\"red\")\n",
    "\n",
    "\n",
    "            shortest_path_length_matrix = pd.DataFrame(index = G.nodes, columns = G.nodes)\n",
    "\n",
    "            for i in G.nodes:\n",
    "                for j in G.nodes:\n",
    "                    if np.isnan(normalized_tm.loc[i][j]):\n",
    "                        continue\n",
    "                    if i==j:\n",
    "                        shortest_path_length_matrix.loc[i][j] = G[i][j]['weight']\n",
    "                    elif i!=j:\n",
    "                        shortest_path_length_matrix.loc[i][j] = nx.shortest_path_length(G,i,j,weight = 'weight')\n",
    "\n",
    "\n",
    "            shortest_path_length_list[effective_length_unique[n]] = shortest_path_length_matrix\n",
    "            G_list[effective_length_unique[n]] = G\n",
    "            Dg_list[effective_length_unique[n]] = shortest_path_length_matrix.max().max()\n",
    "        return shortest_path_length_list,G_list,Dg_list\n",
    "    elif method == 'aggregate':\n",
    "        normalized_tm = transition_matrix\n",
    "        # Developing the transition graphs for each effective length in networkx\n",
    "        projectType = normalized_tm['index'].unique()\n",
    "        G = nx.DiGraph()\n",
    "        none = [G.add_node(k) for k in projectType]\n",
    "        normalized_tm = normalized_tm.set_index('index')\n",
    "        normalized_tm.columns = pd.to_numeric(normalized_tm.columns)\n",
    "        for i in range(len(normalized_tm.index)):\n",
    "            nodes = normalized_tm.loc[normalized_tm.index[i]][normalized_tm.loc[normalized_tm.index[i]].notna()].index.values\n",
    "            for k in nodes:\n",
    "                G.add_edge(normalized_tm.index[i],k,weight=normalized_tm.loc[normalized_tm.index[i]][k])  \n",
    "#         plt.figure(figsize=(5,5))\n",
    "        pos = nx.shell_layout(G)\n",
    "        degree = dict(G.degree)\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] for u,v in edges]\n",
    "#         nx.draw(G, pos, with_labels = True,node_size=[v * 100 for v in degree.values()], node_color=\"red\")\n",
    "\n",
    "\n",
    "        shortest_path_length_matrix = pd.DataFrame(index = G.nodes, columns = G.nodes)\n",
    "\n",
    "        for i in G.nodes:\n",
    "            for j in G.nodes:\n",
    "                if np.isnan(normalized_tm.loc[i][j]):\n",
    "                    continue\n",
    "                if i==j:\n",
    "                    shortest_path_length_matrix.loc[i][j] = G[i][j]['weight']\n",
    "                elif i!=j:\n",
    "                    shortest_path_length_matrix.loc[i][j] = nx.shortest_path_length(G,i,j,weight = 'weight')\n",
    "\n",
    "\n",
    "        Dg = shortest_path_length_matrix.max().max()\n",
    "        return shortest_path_length_matrix,G,Dg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0aaf3b",
   "metadata": {},
   "source": [
    "# Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34f4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryTrajectory(testData,queryClient,points,n):\n",
    "    print('Preparing the query trajectories')\n",
    "    if points == 'exit':\n",
    "        # Test client's trajectories as defined Tc = [(p1,(s1,e1)),(p2,(s2,e2)),..,(pN,(sN,eN))] --> list of tuples\n",
    "        query_trajectories = {}\n",
    "        for i in range(len(queryClient)):\n",
    "            temp = testData[testData['ClientID']==queryClient[i]]\n",
    "            T = []\n",
    "            for j in range(len(temp)):\n",
    "                T = T + [(temp['ProjectType'].iloc[j],(temp['EntryDate'].iloc[j],temp['ExitDate'].iloc[j]))]\n",
    "                if j == len(temp)-1:\n",
    "                    T = T + [(-3)]\n",
    "            query_trajectories[queryClient[i]] = T   \n",
    "    elif points == 'interim':\n",
    "        # Test client's trajectories as defined Tc = [(p1,(s1,e1)),(p2,(s2,e2)),..,(pN,(sN,eN))] --> list of tuples\n",
    "        query_trajectories = {}\n",
    "        for i in range(len(queryClient)):\n",
    "            temp = testData[testData['ClientID']==queryClient[i]]\n",
    "            T = []\n",
    "            for j in range(len(temp)):\n",
    "                T = T + [(temp['ProjectType'].iloc[j],(temp['EntryDate'].iloc[j],temp['ExitDate'].iloc[j]))]\n",
    "            query_trajectories[queryClient[i]] = T  \n",
    "        \n",
    "    # Separating the query history and target for prediction\n",
    "    # query_chopped_client = query_chopped_trajectory.keys()\n",
    "    query_past = {}\n",
    "    query_target = {}\n",
    "    for i in queryClient:\n",
    "        temp = query_trajectories[i]\n",
    "        if len(temp)>1:\n",
    "            query_past[i] = temp[0:len(temp)-1]\n",
    "            if type(temp[-1]) == tuple:\n",
    "                query_target[i] = temp[-1][0]\n",
    "            else: \n",
    "                query_target[i] = temp[-1]\n",
    "    \n",
    "    query_past_chopped = {}\n",
    "    for i in query_past.keys():\n",
    "        temp = query_past[i]\n",
    "        if len(temp)>n:\n",
    "            temp1 = [temp[i] for i in range(n-1,-1,-1)] \n",
    "            temp1.reverse()\n",
    "        else:\n",
    "            temp1 = temp\n",
    "        query_past_chopped[i] = temp1\n",
    "        sNminusn = temp1[0][1][0]\n",
    "        eN = temp1[-1][1][1]\n",
    "    \n",
    "    return query_past_chopped,query_target \n",
    "\n",
    "def HistClientWithinTimeInterval(query_past,historicalClient,testData,n,method = 'others'):\n",
    "    print('Extracting the historical clients within the time interval')\n",
    "    # Determining the queries time interval (s_(N-n),e_N) and the histroical data within that time interval\n",
    "    b = 0\n",
    "    temp_hist = pd.DataFrame()\n",
    "    next_target = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for i in query_past_chopped.keys():\n",
    "        print(b)\n",
    "        temp1 = query_past_chopped[i]\n",
    "        sNminusn = temp1[0][1][0]\n",
    "        eN = temp1[-1][1][1]\n",
    "        if method == 'others':\n",
    "            for j in range(len(historicalClient)):\n",
    "                temp2 = testData[testData['ClientID']==historicalClient[j]]\n",
    "                temp2_updated = pd.DataFrame()\n",
    "                if temp2['EntryDate'].iloc[0] >= eN or temp2['ExitDate'].iloc[-1] <= sNminusn:\n",
    "                    continue\n",
    "                for k in temp2.index:\n",
    "                    if temp2['EntryDate'].loc[k]>=sNminusn and temp2['ExitDate'].loc[k]<=eN:\n",
    "                        temp2_updated = temp2_updated.append(temp2.loc[k])\n",
    "                        if k == temp2.index[-1]:\n",
    "                            temp2_updated = temp2_updated.append({'ClientID':historicalClient[j],'ProjectType':-3},ignore_index=True)\n",
    "                    elif temp2['EntryDate'].loc[k]<=sNminusn and temp2['ExitDate'].loc[k]>=sNminusn and temp2['ExitDate'].loc[k]<=eN:\n",
    "                        temp2.at[k,'EntryDate'] = sNminusn\n",
    "                        temp2_updated = temp2_updated.append(temp2.loc[k])\n",
    "                        if k == temp2.index[-1]:\n",
    "                            temp2_updated = temp2_updated.append({'ClientID':historicalClient[j],'ProjectType':-3},ignore_index=True)\n",
    "                    elif temp2['EntryDate'].loc[k]>=sNminusn and temp2['EntryDate'].loc[k]<=eN and temp2['ExitDate'].loc[k]>=eN:\n",
    "                        temp2.at[k,'ExitDate'] = eN\n",
    "                        temp2_updated = temp2_updated.append(temp2.loc[k])\n",
    "                        if k == temp2.index[-1]:\n",
    "                            temp2_updated = temp2_updated.append({'ClientID':historicalClient[j],'ProjectType':-3},ignore_index=True)\n",
    "                    elif temp2['EntryDate'].loc[k]<=sNminusn and temp2['ExitDate'].loc[k]>=eN:\n",
    "                        temp2.at[k,'EntryDate'] = sNminusn\n",
    "                        temp2.at[k,'ExitDate'] = eN\n",
    "                        temp2_updated = temp2_updated.append(temp2.loc[k])    \n",
    "                        if k == temp2.index[-1]:\n",
    "                            temp2_updated = temp2_updated.append({'ClientID':historicalClient[j],'ProjectType':-3},ignore_index=True)\n",
    "\n",
    "                temp2_updated['query client'] = [i for a in range(len(temp2_updated))]\n",
    "                temp_hist = temp_hist.append(temp2_updated)\n",
    "         \n",
    "        elif method == 'baseline 1':\n",
    "            for j in range(len(historicalClient)):\n",
    "                temp2 = testData[testData['ClientID']==historicalClient[j]].reset_index(drop=True)\n",
    "                temp2_updated = pd.DataFrame()\n",
    "                if temp2['EntryDate'].iloc[0] >= eN or temp2['ExitDate'].iloc[-1] <= sNminusn:\n",
    "                    continue\n",
    "                for k in range(len(temp2)):\n",
    "                    if temp2['EntryDate'].iloc[k]>=sNminusn and temp2['ExitDate'].iloc[k]<=eN:\n",
    "                        temp2_updated = temp2_updated.append(temp2.iloc[k])\n",
    "                        if k == len(temp2)-1:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":-3},ignore_index=True)\n",
    "                        else:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k+1]},ignore_index=True)\n",
    "\n",
    "\n",
    "                    elif temp2['EntryDate'].iloc[k]<=sNminusn and temp2['ExitDate'].iloc[k]>=sNminusn and temp2['ExitDate'].iloc[k]<=eN:\n",
    "                        temp2.at[k,'EntryDate'] = sNminusn\n",
    "                        temp2_updated = temp2_updated.append(temp2.iloc[k])\n",
    "                        if k == len(temp2)-1:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":-3},ignore_index=True)\n",
    "                        else:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k+1]},ignore_index=True)\n",
    "\n",
    "                    elif temp2['EntryDate'].iloc[k]>=sNminusn and temp2['EntryDate'].iloc[k]<=eN and temp2['ExitDate'].iloc[k]>=eN:\n",
    "                        temp2.at[k,'ExitDate'] = eN\n",
    "                        temp2_updated = temp2_updated.append(temp2.iloc[k])\n",
    "                        if temp2['ExitDate'].iloc[k]>eN:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k]},ignore_index=True)\n",
    "                        elif temp2['ExitDate'].iloc[k] == eN:\n",
    "                            if k == len(temp2)-1:\n",
    "                                next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":-3},ignore_index=True)\n",
    "                            else:\n",
    "                                next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k+1]},ignore_index=True)\n",
    "\n",
    "                    elif temp2['EntryDate'].iloc[k]<=sNminusn and temp2['ExitDate'].iloc[k]>=eN:\n",
    "                        temp2.at[k,'EntryDate'] = sNminusn\n",
    "                        temp2.at[k,'ExitDate'] = eN\n",
    "                        temp2_updated = temp2_updated.append(temp2.iloc[k])   \n",
    "                        if temp2['ExitDate'].iloc[k]>eN:\n",
    "                            next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k]},ignore_index=True)\n",
    "                        elif temp2['ExitDate'].iloc[k] == eN:\n",
    "                            if k == len(temp2)-1:\n",
    "                                next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":-3},ignore_index=True)\n",
    "                            else:\n",
    "                                next_target = next_target.append({'query client':i,\n",
    "                                                              'historical client':historicalClient[j],\n",
    "                                                              \"next_node\":temp2['ProjectType'].iloc[k+1]},ignore_index=True)\n",
    "                temp2_updated['query client'] = [i for a in range(len(temp2_updated))]\n",
    "                temp_hist = temp_hist.append(temp2_updated)\n",
    "        b = b+1\n",
    "    if method == 'others':\n",
    "        return temp_hist\n",
    "    elif method == 'baseline 1':\n",
    "        return temp_hist,next_target\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b9825",
   "metadata": {},
   "source": [
    "# Similarity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b35e17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Overlap(query_past_chopped,temp_hist):\n",
    "    print(\"Computing overlap\")\n",
    "    # Computing overlap for each test query\n",
    "    testingQueryClientUnique = temp_hist['query client'].unique()\n",
    "    overlap = pd.DataFrame()\n",
    "    for a in range(len(testingQueryClientUnique)):\n",
    "        print(a)\n",
    "        temp = temp_hist[temp_hist['query client']==testingQueryClientUnique[a]]\n",
    "        historicalClientUnique = temp['ClientID'].unique()\n",
    "        for b in range(len(historicalClientUnique)):\n",
    "            temp1 = temp[temp['ClientID']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                q = testingQueryTrajectory[k][0]\n",
    "                for m in range(len(temp1)):\n",
    "    #                 print(tq,temp1.iloc[m]['EntryDate'],temp1.iloc[m]['ExitDate'])\n",
    "                    if temp1.iloc[m]['EntryDate']>=tq[0] and temp1.iloc[m]['ExitDate']<=tq[1]:\n",
    "\n",
    "                        overlap_value = abs(temp1.iloc[m]['ExitDate']-temp1.iloc[m]['EntryDate'])\n",
    "                        if temp1.iloc[m]['ExitDate']==temp1.iloc[m]['EntryDate']:\n",
    "                            overlap_value = timedelta(days=1)\n",
    "\n",
    "                        overlap = overlap.append({'query client':testingQueryClientUnique[a],\n",
    "                                                  'historical client':historicalClientUnique[b],\n",
    "                                                  \"overlap initial\":temp1.iloc[m]['EntryDate'],\n",
    "                                                  \"overlap final\":temp1.iloc[m]['ExitDate'],\n",
    "                                                  \"overlap\": overlap_value,\n",
    "                                                  'q':q,\n",
    "                                                  'node':temp1.iloc[m]['ProjectType']},ignore_index = True)\n",
    "                    elif temp1.iloc[m]['EntryDate']<tq[0] and temp1.iloc[m]['ExitDate']>tq[0] and temp1.iloc[m]['ExitDate']<=tq[1]:\n",
    "\n",
    "                        overlap_value = abs(temp1.iloc[m]['ExitDate']-tq[0])\n",
    "                        if temp1.iloc[m]['ExitDate']==tq[0]:\n",
    "                            overlap_value = timedelta(days=1)\n",
    "                        overlap = overlap.append({'query client':testingQueryClientUnique[a],\n",
    "                                                  'historical client':historicalClientUnique[b],\n",
    "                                                  \"overlap initial\":tq[0],\n",
    "                                                  \"overlap final\":temp1.iloc[m]['ExitDate'],\n",
    "                                                  \"overlap\": overlap_value,\n",
    "                                                  \"q\":q,\n",
    "                                                  'node':temp1.iloc[m]['ProjectType']},ignore_index = True)            \n",
    "                    elif temp1.iloc[m]['EntryDate']>=tq[0] and temp1.iloc[m]['EntryDate']<tq[1] and temp1.iloc[m]['ExitDate']>tq[1]:\n",
    "\n",
    "                        overlap_value = abs(tq[1]-temp1.iloc[m]['EntryDate'])\n",
    "                        if tq[1]==temp1.iloc[m]['EntryDate']:\n",
    "                            overlap_value = timedelta(days=1)\n",
    "                        overlap = overlap.append({'query client':testingQueryClientUnique[a],\n",
    "                                                  'historical client':historicalClientUnique[b],\n",
    "                                                  \"overlap initial\":temp1.iloc[m]['EntryDate'],\n",
    "                                                  \"overlap final\":tq[1],\n",
    "                                                  \"overlap\": overlap_value,\n",
    "                                                  'q':q,\n",
    "                                                  'node':temp1.iloc[m]['ProjectType']},ignore_index = True)                        \n",
    "                    elif temp1.iloc[m]['EntryDate']<tq[0] and temp1.iloc[m]['ExitDate']>tq[1]:\n",
    "\n",
    "                        overlap_value = abs(tq[1]-tq[0])\n",
    "                        if tq[1]==tq[0]:\n",
    "                            overlap_value = timedelta(days=1)\n",
    "                        overlap = overlap.append({'query client':testingQueryClientUnique[a],\n",
    "                                                  'historical client':historicalClientUnique[b],\n",
    "                                                  \"overlap initial\":tq[0],\n",
    "                                                  \"overlap final\":tq[1],\n",
    "                                                  \"overlap\" : overlap_value,\n",
    "                                                  'q':q,\n",
    "                                                  'node':temp1.iloc[m]['ProjectType']},ignore_index = True)  \n",
    "\n",
    "    overlap['overlap'] = overlap['overlap'].dt.days\n",
    "    overlap = overlap.sort_values([\"overlap initial\",'overlap final']).reset_index(drop=True)\n",
    "    return overlap\n",
    "\n",
    "def SimilarityInfoComputation(temp_hist,overlap,method):\n",
    "    print(\"Computing Similarity info\")\n",
    "    if method == 'method 1':\n",
    "        testingQueryClientUnique = temp_hist['query client'].unique()\n",
    "        similarity_info = pd.DataFrame()\n",
    "        minDistanceData = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = overlap[overlap['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                q = testingQueryTrajectory[k][0]\n",
    "                overlapping_interval = temp[(temp['overlap initial']>=tq[0]) & (temp['overlap final']<=tq[1])]\n",
    "                overlapping_interval_hist_client = overlapping_interval['historical client'].unique()\n",
    "                for l in range(len(overlapping_interval_hist_client)):\n",
    "                    temp1 = overlapping_interval[overlapping_interval['historical client']==overlapping_interval_hist_client[l]]\n",
    "                    maxOverlap = temp1.loc[temp1['overlap']==temp1['overlap'].max()]\n",
    "                    maxOverlap = maxOverlap.reset_index(drop=True)\n",
    "                    for i in range(len(maxOverlap)):\n",
    "                        maxOverlap.at[i,'distance'] = shortest_path_length_list[trajectory_effective_length].loc[maxOverlap.iloc[i]['q']][maxOverlap.iloc[i]['node']]/Dg_list[trajectory_effective_length]\n",
    "                    maxOverlap['distance'] = maxOverlap['distance'].fillna(10000)\n",
    "                    minDistance = maxOverlap.loc[maxOverlap['distance']==maxOverlap['distance'].min()].iloc[-1]\n",
    "                    minDistanceData = minDistanceData.append(minDistance)\n",
    "        similarity_info = minDistanceData\n",
    "        similarity_info = similarity_info.drop_duplicates(keep='first')\n",
    "    \n",
    "        return similarity_info\n",
    "    elif method == 'method 2':\n",
    "        # Determining the max overlap \n",
    "        testingQueryClientUnique = temp_hist['query client'].unique()\n",
    "        similarity_info = pd.DataFrame()\n",
    "        minDistanceData_l1 = pd.DataFrame()\n",
    "        minDistanceData_l2 = pd.DataFrame()\n",
    "\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = overlap[overlap['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                q = testingQueryTrajectory[k][0]\n",
    "                overlapping_interval = temp[(temp['overlap initial']>=tq[0]) & (temp['overlap final']<=tq[1])]\n",
    "                overlapping_interval_hist_client = overlapping_interval['historical client'].unique()\n",
    "                for l in range(len(overlapping_interval_hist_client)):\n",
    "                    temp1 = overlapping_interval[overlapping_interval['historical client']==overlapping_interval_hist_client[l]]\n",
    "                    maxOverlap = temp1.loc[temp1['overlap']==temp1['overlap'].max()]\n",
    "                    maxOverlap = maxOverlap.reset_index(drop=True)\n",
    "                    for i in range(len(maxOverlap)):\n",
    "                        maxOverlap.at[i,'distance_l'] = shortest_path_length_list[trajectory_effective_length].loc[maxOverlap.iloc[i]['q']][maxOverlap.iloc[i]['node']]/Dg_list[trajectory_effective_length]\n",
    "                        maxOverlap.at[i,'distance_l+1'] = shortest_path_length_list[trajectory_effective_length+1].loc[maxOverlap.iloc[i]['q']][maxOverlap.iloc[i]['node']]/Dg_list[trajectory_effective_length+1]\n",
    "\n",
    "                    maxOverlap['distance_l'] = maxOverlap['distance_l'].fillna(10000)\n",
    "                    maxOverlap['distance_l+1'] = maxOverlap['distance_l+1'].fillna(10000)\n",
    "\n",
    "                    minDistance_l1 = maxOverlap.loc[maxOverlap['distance_l']==maxOverlap['distance_l'].min()].iloc[-1]\n",
    "                    minDistanceData_l1 = minDistanceData_l1.append(minDistance_l1)\n",
    "\n",
    "                    minDistance_l2 = maxOverlap.loc[maxOverlap['distance_l+1']==maxOverlap['distance_l+1'].min()].iloc[-1]\n",
    "                    minDistanceData_l2 = minDistanceData_l2.append(minDistance_l2)\n",
    "\n",
    "        similarity_info_l1 = minDistanceData_l1 \n",
    "        similarity_info_l2 = minDistanceData_l2\n",
    "        similarity_info_l1 = similarity_info_l1.drop_duplicates(keep='first')\n",
    "        similarity_info_l2 = similarity_info_l2.drop_duplicates(keep='first')\n",
    "        return similarity_info_l1,similarity_info_l2\n",
    "    \n",
    "    elif method == 'baseline':\n",
    "        testingQueryClientUnique = temp_hist['query client'].unique()\n",
    "        minDistanceData = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = overlap[overlap['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "        #     trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                q = testingQueryTrajectory[k][0]\n",
    "                overlapping_interval = temp[(temp['overlap initial']>=tq[0]) & (temp['overlap final']<=tq[1])]\n",
    "                overlapping_interval_hist_client = overlapping_interval['historical client'].unique()\n",
    "                for l in range(len(overlapping_interval_hist_client)):\n",
    "                    temp1 = overlapping_interval[overlapping_interval['historical client']==overlapping_interval_hist_client[l]]\n",
    "                    maxOverlap = temp1.loc[temp1['overlap']==temp1['overlap'].max()]\n",
    "                    maxOverlap = maxOverlap.reset_index(drop=True)\n",
    "                    for i in range(len(maxOverlap)):\n",
    "                        maxOverlap.at[i,'distance'] = shortest_path_length_matrix.loc[maxOverlap.iloc[i]['q']][maxOverlap.iloc[i]['node']]/Dg\n",
    "                    maxOverlap['distance'] = maxOverlap['distance'].fillna(10000)\n",
    "                    minDistance = maxOverlap.loc[maxOverlap['distance']==maxOverlap['distance'].min()].iloc[-1]\n",
    "                    minDistanceData = minDistanceData.append(minDistance)\n",
    "        similarity_info = minDistanceData\n",
    "        return similarity_info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SimilarityComputation(similarity_info,method):\n",
    "    print('Computing similarity')\n",
    "    if method == 'method 1':\n",
    "        similarity_info = similarity_info[0]\n",
    "        # Computing the similarity between trajectories\n",
    "        testingQueryClientUnique = similarity_info['query client'].unique()\n",
    "        similarity = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity_info[similarity_info['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            historicalClientUnique = temp['historical client'].unique()\n",
    "            similarity_denum = 0\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                if tq[0]==tq[1]:\n",
    "                    overlapped = 1\n",
    "                else: \n",
    "                    overlapped = (tq[1]-tq[0]).days\n",
    "                similarity_denum = similarity_denum + overlapped\n",
    "\n",
    "            for b in range(len(historicalClientUnique)):\n",
    "                temp1 = temp[temp['historical client']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "                temp1 = temp1.sort_values(\"overlap initial\").reset_index(drop=True)\n",
    "                trac = list(temp1['node'])\n",
    "                similarity_sum = 0\n",
    "                for k in range(len(temp1)):\n",
    "                    overlapPerTimeInterval = temp1.iloc[k][\"overlap\"]\n",
    "                    d = temp1.iloc[k][\"distance\"]\n",
    "                    similarity_sum = similarity_sum + (overlapPerTimeInterval*np.exp(-d))/similarity_denum\n",
    "\n",
    "                similarity = similarity.append({\"historical client\":historicalClientUnique[b],\n",
    "                                                \"query client\":testingQueryClientUnique[a],\n",
    "                                               \"historical trajectory\":trac,\n",
    "                                                \"historical distance\": list(temp1['distance'].values),\n",
    "                                                \"historical overlap\": list(temp1['overlap']),\n",
    "                                                \"historical initial\": list(temp1['overlap initial'].dt.date),\n",
    "                                                \"historical final\": list(temp1['overlap final'].dt.date),\n",
    "                                                \"length_historical trajectory\":len(trac),\n",
    "                                                \"length_query trajectory\":len(testingQueryTrajectory),\n",
    "                                                'overlap': temp1['overlap'].sum(),\n",
    "                                               \"similarity\":similarity_sum},ignore_index=True)  \n",
    "    elif method == 'method 2':\n",
    "        similarity_info_l1 = similarity_info[0]\n",
    "        similarity_info_l2 = similarity_info[1]\n",
    "        # Computing the similarity between trajectories using minimum distance for l\n",
    "        print('Computing similarity for l')\n",
    "        testingQueryClientUnique = similarity_info_l1['query client'].unique()\n",
    "        similarity_l1 = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity_info_l1[similarity_info_l1['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            historicalClientUnique = temp['historical client'].unique()\n",
    "            similarity_denum = 0\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                if tq[0]==tq[1]:\n",
    "                    overlapped = 1\n",
    "                else: \n",
    "                    overlapped = (tq[1]-tq[0]).days\n",
    "                similarity_denum = similarity_denum + overlapped\n",
    "            for b in range(len(historicalClientUnique)):\n",
    "                temp1 = temp[temp['historical client']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "                temp1 = temp1.sort_values(\"overlap initial\").reset_index(drop=True)\n",
    "                trac = list(temp1['node'])\n",
    "                similarity_sum = 0\n",
    "                for k in range(len(temp1)):\n",
    "                    overlapPerTimeInterval = temp1.iloc[k][\"overlap\"]\n",
    "                    d = temp1.iloc[k][\"distance_l\"]\n",
    "                    similarity_sum = similarity_sum + (overlapPerTimeInterval*np.exp(-d))/similarity_denum\n",
    "\n",
    "                similarity_l1 = similarity_l1.append({\"historical client\":historicalClientUnique[b],\n",
    "                                                \"query client\":testingQueryClientUnique[a],\n",
    "                                               \"historical trajectory\":trac,\n",
    "                                                \"historical distance\": list(temp1['distance_l'].values),\n",
    "                                                \"historical overlap\": list(temp1['overlap']),\n",
    "                                                \"historical initial\": list(temp1['overlap initial'].dt.date),\n",
    "                                                \"historical final\": list(temp1['overlap final'].dt.date),\n",
    "                                                \"effective length_historical trajectory\":len(trac),\n",
    "                                                \"length_query trajectory\":len(testingQueryTrajectory),\n",
    "                                                'overlap': temp1['overlap'].sum(),\n",
    "                                               \"similarity\":similarity_sum},ignore_index=True)  \n",
    "        \n",
    "\n",
    "        # Computing the similarity between trajectories using minimum distance for l+1\n",
    "        print('Computing similarity for l+1')\n",
    "        testingQueryClientUnique = similarity_info_l2['query client'].unique()\n",
    "        similarity_l2 = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity_info_l2[similarity_info_l2['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            historicalClientUnique = temp['historical client'].unique()\n",
    "            similarity_denum = 0\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                if tq[0]==tq[1]:\n",
    "                    overlapped = 1\n",
    "                else: \n",
    "                    overlapped = (tq[1]-tq[0]).days\n",
    "                similarity_denum = similarity_denum + overlapped\n",
    "            for b in range(len(historicalClientUnique)):\n",
    "                temp1 = temp[temp['historical client']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "                temp1 = temp1.sort_values(\"overlap initial\").reset_index(drop=True)\n",
    "                trac = list(temp1['node'])\n",
    "                similarity_sum = 0\n",
    "                for k in range(len(temp1)):\n",
    "                    overlapPerTimeInterval = temp1.iloc[k][\"overlap\"]\n",
    "                    d = temp1.iloc[k][\"distance_l+1\"]\n",
    "                    similarity_sum = similarity_sum + (overlapPerTimeInterval*np.exp(-d))/similarity_denum\n",
    "\n",
    "                similarity_l2 = similarity_l2.append({\"historical client\":historicalClientUnique[b],\n",
    "                                                \"query client\":testingQueryClientUnique[a],\n",
    "                                               \"historical trajectory\":trac,\n",
    "                                                \"historical distance\": list(temp1['distance_l+1'].values),\n",
    "                                                \"historical overlap\": list(temp1['overlap']),\n",
    "                                                \"historical initial\": list(temp1['overlap initial'].dt.date),\n",
    "                                                \"historical final\": list(temp1['overlap final'].dt.date),\n",
    "                                                \"effective length_historical trajectory\":len(trac),\n",
    "                                                \"length_query trajectory\":len(testingQueryTrajectory),\n",
    "                                                'overlap': temp1['overlap'].sum(),\n",
    "                                               \"similarity\":similarity_sum},ignore_index=True)     \n",
    "      \n",
    "\n",
    "        similarity = [similarity_l1,similarity_l2]\n",
    "    elif method == 'baseline':\n",
    "        similarity_info = similarity_info[0]\n",
    "        # Computing the similarity between trajectories\n",
    "        testingQueryClientUnique = similarity_info['query client'].unique()\n",
    "        similarity = pd.DataFrame()\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity_info[similarity_info['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            historicalClientUnique = temp['historical client'].unique()\n",
    "            similarity_denum = 0\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                if tq[0]==tq[1]:\n",
    "                    overlapped = 1\n",
    "                else: \n",
    "                    overlapped = (tq[1]-tq[0]).days\n",
    "                similarity_denum = similarity_denum + overlapped\n",
    "\n",
    "            for b in range(len(historicalClientUnique)):\n",
    "                temp1 = temp[temp['historical client']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "                temp1 = temp1.sort_values(\"overlap initial\").reset_index(drop=True)\n",
    "                trac = list(temp1['node'])\n",
    "                similarity_sum = 0\n",
    "                for k in range(len(temp1)):\n",
    "                    overlapPerTimeInterval = temp1.iloc[k][\"overlap\"]\n",
    "                    d = temp1.iloc[k][\"distance\"]\n",
    "                    similarity_sum = similarity_sum + (overlapPerTimeInterval*np.exp(-d))/similarity_denum\n",
    "\n",
    "                similarity = similarity.append({\"historical client\":historicalClientUnique[b],\n",
    "                                                \"query client\":testingQueryClientUnique[a],\n",
    "                                               \"historical trajectory\":trac,\n",
    "                                                \"historical distance\": list(temp1['distance'].values),\n",
    "                                                \"historical overlap\": list(temp1['overlap']),\n",
    "                                                \"historical initial\": list(temp1['overlap initial'].dt.date),\n",
    "                                                \"historical final\": list(temp1['overlap final'].dt.date),\n",
    "                                                \"length_historical trajectory\":len(trac),\n",
    "                                                \"length_query trajectory\":len(testingQueryTrajectory),\n",
    "                                                'overlap': temp1['overlap'].sum(),\n",
    "                                               \"similarity\":similarity_sum},ignore_index=True)  \n",
    "\n",
    " \n",
    "    elif method == 'baseline 6':\n",
    "        similarity_info = similarity_info[0]\n",
    "        # Computing the similarity between trajectories\n",
    "        testingQueryClientUnique = similarity_info['query client'].unique()\n",
    "        similarity = pd.DataFrame()\n",
    "        beta = 0.8\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity_info[similarity_info['query client']==testingQueryClientUnique[a]]\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            historicalClientUnique = temp['historical client'].unique()\n",
    "            similarity_denum = 0\n",
    "            for k in range(len(testingQueryTrajectory)-1,-1,-1):\n",
    "                tq = testingQueryTrajectory[k][1]\n",
    "                if tq[0]==tq[1]:\n",
    "                    overlapped = 1\n",
    "                else: \n",
    "                    overlapped = (tq[1]-tq[0]).days\n",
    "                similarity_denum = similarity_denum + overlapped\n",
    "\n",
    "            for b in range(len(historicalClientUnique)):\n",
    "                temp1 = temp[temp['historical client']==historicalClientUnique[b]].reset_index(drop=True)\n",
    "                temp1 = temp1.sort_values(\"overlap initial\").reset_index(drop=True)\n",
    "                trac = list(temp1['node'])\n",
    "                similarity_sum = 0\n",
    "                for k in range(len(temp1)):\n",
    "                    overlapPerTimeInterval = temp1.iloc[k][\"overlap\"]\n",
    "                    d = temp1.iloc[k][\"distance\"]\n",
    "                    similarity_sum = similarity_sum + (beta**(len(temp1)-k-1)*overlapPerTimeInterval*np.exp(-d))/similarity_denum\n",
    "                similarity = similarity.append({\"historical client\":historicalClientUnique[b],\n",
    "                                                \"query client\":testingQueryClientUnique[a],\n",
    "                                               \"historical trajectory\":trac,\n",
    "                                                \"historical distance\": list(temp1['distance'].values),\n",
    "                                                \"historical overlap\": list(temp1['overlap']),\n",
    "                                                \"historical initial\": list(temp1['overlap initial'].dt.date),\n",
    "                                                \"historical final\": list(temp1['overlap final'].dt.date),\n",
    "                                                \"length_historical trajectory\":len(trac),\n",
    "                                                \"length_query trajectory\":len(testingQueryTrajectory),\n",
    "                                                'overlap': temp1['overlap'].sum(),\n",
    "                                               \"similarity\":similarity_sum},ignore_index=True)  \n",
    "    \n",
    " \n",
    "    return similarity\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "def Prediction(similarity,method,k=10):\n",
    "    print('Predicting')\n",
    "    if method == 'method 1':\n",
    "        similarity = similarity[0]\n",
    "        testingQueryClientUnique = similarity['query client'].unique()\n",
    "        prediction = pd.DataFrame(index = range(len(testingQueryClientUnique)),columns=['ClientID','actual']+['Prediction@'+str(i+1) for i in range(k)])\n",
    "\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity[similarity['query client']==testingQueryClientUnique[a]]\n",
    "            temp = temp.sort_values(by=['similarity'],ascending=False)\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            testingQueryTarget = query_target[testingQueryClientUnique[a]]\n",
    "            testing_trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            qn = testingQueryTrajectory[-1][0]\n",
    "            NB_q = set([n for n in G_list[testing_trajectory_effective_length].neighbors(qn)])\n",
    "            prediction.at[a,'ClientID'] = testingQueryClientUnique[a]\n",
    "            prediction.at[a,'actual'] = testingQueryTarget\n",
    "\n",
    "            temp = temp.sort_values('similarity',ascending=False)\n",
    "            maxSimilarity = pd.DataFrame()\n",
    "            i = 0\n",
    "            maxSimilarity = maxSimilarity.append(temp.iloc[i])\n",
    "         \n",
    "\n",
    "            for j in range(1,len(temp)):\n",
    "                if i >=k:\n",
    "                    break\n",
    "                if temp['historical trajectory'].iloc[j] == maxSimilarity.iloc[i]['historical trajectory'] and temp['similarity'].iloc[j] == maxSimilarity.iloc[i]['similarity']:\n",
    "                    continue\n",
    "                else:\n",
    "                    i = i+1\n",
    "                    if i<k:\n",
    "                        maxSimilarity = maxSimilarity.append(temp.iloc[j])\n",
    "            maxSimilarity = maxSimilarity.reset_index(drop=True)\n",
    "            maxSimilarity = maxSimilarity.sort_values('similarity',ascending=False)\n",
    "            \n",
    "            for i in range(k):\n",
    "                if i>=len(maxSimilarity):\n",
    "                    continue\n",
    "                similar_historical_trajectory = maxSimilarity.iloc[i]['historical trajectory']\n",
    "                similar_trajectory_effective_length = len(pd.DataFrame(similar_historical_trajectory)[0].unique())\n",
    "                pn = similar_historical_trajectory[-1]\n",
    "                NB_p = set([m for m in G_list[similar_trajectory_effective_length].neighbors(pn)])\n",
    "                NBp_NBq_intersect = NB_p.intersection(NB_q)\n",
    "                pntoNBp_NBq_intersect = {}\n",
    "                for b in NBp_NBq_intersect:\n",
    "                    pntoNBp_NBq_intersect[('p',b)] = G_list[similar_trajectory_effective_length][pn][b][\"weight\"]\n",
    "                if not pntoNBp_NBq_intersect:\n",
    "                    continue\n",
    "                maxProbNode = max(pntoNBp_NBq_intersect,key=pntoNBp_NBq_intersect.get)\n",
    "                \n",
    "                testingQueryPred = maxProbNode[1]\n",
    "                prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "    elif method == 'method 2':\n",
    "        \n",
    "        similarity_l1 = similarity[0]\n",
    "        similarity_l2 = similarity[1]\n",
    "        # Computing the prediction of test queries using two of the similarities\n",
    "        testingQueryClientUnique = similarity_l1['query client'].unique()\n",
    "        prediction = pd.DataFrame(index = range(len(testingQueryClientUnique)),columns=['ClientID','actual']+['Prediction@'+str(i+1) for i in range(k)])\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp1 = similarity_l1[similarity_l1['query client']==testingQueryClientUnique[a]]\n",
    "            temp1 = temp1.sort_values(by=['similarity'],ascending=False)\n",
    "            temp2 = similarity_l2[similarity_l2['query client']==testingQueryClientUnique[a]]\n",
    "            temp2 = temp2.sort_values(by=['similarity'],ascending=False)\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            testingQueryTarget = query_target[testingQueryClientUnique[a]]\n",
    "            testing_trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            qn = testingQueryTrajectory[-1][0]\n",
    "            NB_q = set([n for n in G_list[testing_trajectory_effective_length].neighbors(qn)])\n",
    "            prediction.at[a,'ClientID'] = testingQueryClientUnique[a]\n",
    "            prediction.at[a,'actual'] = testingQueryTarget\n",
    "\n",
    "            temp1 = temp1.sort_values('similarity',ascending=False)\n",
    "            maxSimilarity_l1 = pd.DataFrame()\n",
    "            i = 0\n",
    "            maxSimilarity_l1 = maxSimilarity_l1.append(temp1.iloc[i])\n",
    "            for j in range(1,len(temp1)):\n",
    "                if i >=k:\n",
    "                    break\n",
    "                if temp1['historical trajectory'].iloc[j] == maxSimilarity_l1.iloc[i]['historical trajectory'] and temp1['similarity'].iloc[j] == maxSimilarity_l1.iloc[i]['similarity']:\n",
    "                    continue\n",
    "                else:\n",
    "                    i = i+1\n",
    "                    if i<k:\n",
    "                        maxSimilarity_l1 = maxSimilarity_l1.append(temp1.iloc[j])\n",
    "            maxSimilarity_l1 = maxSimilarity_l1.reset_index(drop=True)\n",
    "            maxSimilarity_l1 = maxSimilarity_l1.sort_values('similarity',ascending=False)\n",
    "\n",
    "            temp2 = temp2.sort_values('similarity',ascending=False)\n",
    "            maxSimilarity_l2 = pd.DataFrame()\n",
    "            i = 0\n",
    "            maxSimilarity_l2 = maxSimilarity_l2.append(temp2.iloc[i])\n",
    "            k = 10\n",
    "            for j in range(1,len(temp2)):\n",
    "                if i >=k:\n",
    "                    break\n",
    "                if temp2['historical trajectory'].iloc[j] == maxSimilarity_l2.iloc[i]['historical trajectory'] and temp2['similarity'].iloc[j] == maxSimilarity_l2.iloc[i]['similarity']:\n",
    "                    continue\n",
    "                else:\n",
    "                    i = i+1\n",
    "                    if i<k:\n",
    "                        maxSimilarity_l2 = maxSimilarity_l2.append(temp2.iloc[j])\n",
    "            maxSimilarity_l2 = maxSimilarity_l2.reset_index(drop=True)\n",
    "            maxSimilarity_l2 = maxSimilarity_l2.sort_values('similarity',ascending=False)\n",
    "\n",
    "\n",
    "            maxSimilarity = pd.DataFrame()\n",
    "            maxSimilarity = maxSimilarity.append(maxSimilarity_l1)\n",
    "            maxSimilarity = maxSimilarity.append(maxSimilarity_l2)\n",
    "            maxSimilarity = maxSimilarity.sort_values(by=['similarity'], ascending=False)\n",
    "\n",
    "            for i in range(k):\n",
    "                if i>=len(maxSimilarity):\n",
    "                    continue\n",
    "                similar_historical_trajectory = maxSimilarity.iloc[i]['historical trajectory']\n",
    "                similar_trajectory_effective_length = len(pd.DataFrame(similar_historical_trajectory)[0].unique())\n",
    "                pn = similar_historical_trajectory[-1]\n",
    "                NB_p = set([m for m in G_list[similar_trajectory_effective_length].neighbors(pn)])\n",
    "                NBp_NBq_intersect = NB_p.intersection(NB_q)\n",
    "                pntoNBp_NBq_intersect = {}\n",
    "                for b in NBp_NBq_intersect:\n",
    "                    pntoNBp_NBq_intersect[('p',b)] = G_list[similar_trajectory_effective_length][pn][b][\"weight\"]\n",
    "                if not pntoNBp_NBq_intersect:\n",
    "                    continue\n",
    "                maxProbNode = max(pntoNBp_NBq_intersect,key=pntoNBp_NBq_intersect.get)\n",
    "                testingQueryPred = maxProbNode[1]\n",
    "                prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "\n",
    "    else:\n",
    "        similarity = similarity[0]\n",
    "        testingQueryClientUnique = similarity['query client'].unique()\n",
    "        prediction = pd.DataFrame(index = range(len(testingQueryClientUnique)),columns=['ClientID','actual']+['Prediction@'+str(i+1) for i in range(k)])\n",
    "        for a in range(len(testingQueryClientUnique)):\n",
    "            print(a)\n",
    "            temp = similarity[similarity['query client']==testingQueryClientUnique[a]]\n",
    "            temp = temp.sort_values(by=['similarity'],ascending=False)\n",
    "            testingQueryTrajectory = query_past_chopped[testingQueryClientUnique[a]]\n",
    "            testingQueryTarget = query_target[testingQueryClientUnique[a]]\n",
    "            testing_trajectory_effective_length = len(pd.DataFrame([testingQueryTrajectory[n][0] for n in range(len(testingQueryTrajectory))])[0].unique())\n",
    "            qn = testingQueryTrajectory[-1][0]\n",
    "            NB_q = set([n for n in G.neighbors(qn)])\n",
    "            prediction.at[a,'ClientID'] = testingQueryClientUnique[a]\n",
    "            prediction.at[a,'actual'] = testingQueryTarget\n",
    "\n",
    "            temp = temp.sort_values('similarity',ascending=False)\n",
    "            maxSimilarity = pd.DataFrame()\n",
    "            i = 0\n",
    "            maxSimilarity = maxSimilarity.append(temp.iloc[i])\n",
    "            k = 10\n",
    "\n",
    "            for j in range(1,len(temp)):\n",
    "                if i >=k:\n",
    "                    break\n",
    "                if temp['historical trajectory'].iloc[j] == maxSimilarity.iloc[i]['historical trajectory'] and temp['similarity'].iloc[j] == maxSimilarity.iloc[i]['similarity']:\n",
    "                    continue\n",
    "                else:\n",
    "                    i = i+1\n",
    "                    if i<k:\n",
    "                        maxSimilarity = maxSimilarity.append(temp.iloc[j])\n",
    "            maxSimilarity = maxSimilarity.reset_index(drop=True)\n",
    "            maxSimilarity = maxSimilarity.sort_values('similarity',ascending=False)\n",
    "\n",
    "            for i in range(k):\n",
    "                if i>=len(maxSimilarity):\n",
    "                    continue\n",
    "                if method == 'baseline 1':\n",
    "                    testingQueryPred = next_target[(next_target['query client']==testingQueryClientUnique[a])&(next_target['historical client']==maxSimilarity.iloc[i]['historical client'])]['next_node'].values[0]\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                elif method == 'baseline 2':\n",
    "                    similar_historical_trajectory = maxSimilarity.iloc[i]['historical trajectory']\n",
    "                    similar_trajectory_effective_length = len(pd.DataFrame(similar_historical_trajectory)[0].unique())\n",
    "                    pn = similar_historical_trajectory[-1]\n",
    "                    pntoNBp = {}\n",
    "                    for m in G.neighbors(pn):\n",
    "                        pntoNBp[('p',m)] = G[pn][m][\"weight\"]\n",
    "                    maxProbNode = max(pntoNBp,key=pntoNBp.get)\n",
    "                    testingQueryPred = maxProbNode[1]\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                elif method == 'baseline 3':\n",
    "                    similar_historical_trajectory = maxSimilarity.iloc[i]['historical trajectory']\n",
    "                    similar_trajectory_effective_length = len(pd.DataFrame(similar_historical_trajectory)[0].unique())\n",
    "                    pn = similar_historical_trajectory[-1]\n",
    "                    qntoNBq = {}\n",
    "                    for m in G.neighbors(qn):\n",
    "                        qntoNBq[('q',m)] = G[qn][m][\"weight\"]\n",
    "                    maxProbNode = max(qntoNBq,key=qntoNBq.get)\n",
    "                    testingQueryPred = maxProbNode[1]\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                elif method == 'baseline 4a':\n",
    "                    testingQueryPred = np.random.choice(G.nodes)\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                elif method == 'baseline 4b':\n",
    "                    prob = [G.in_degree(node) for node in G.nodes]/np.sum([G.in_degree(node) for node in G.nodes])\n",
    "                    testingQueryPred = np.random.choice(G.nodes,p=prob)\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                elif method == 'baseline 5' or method == 'baseline 6':\n",
    "                    similar_historical_trajectory = maxSimilarity.iloc[i]['historical trajectory']\n",
    "                    similar_trajectory_effective_length = len(pd.DataFrame(similar_historical_trajectory)[0].unique())\n",
    "                    pn = similar_historical_trajectory[-1]\n",
    "                    NB_p = set([m for m in G.neighbors(pn)])\n",
    "                    NBp_NBq_intersect = NB_p.intersection(NB_q)\n",
    "                    pntoNBp_NBq_intersect = {}\n",
    "                    for b in NBp_NBq_intersect:\n",
    "                        pntoNBp_NBq_intersect[('p',b)] = G[pn][b][\"weight\"]\n",
    "                    if not pntoNBp_NBq_intersect:\n",
    "                        continue\n",
    "                    maxProbNode = max(pntoNBp_NBq_intersect,key=pntoNBp_NBq_intersect.get)\n",
    "                    testingQueryPred = maxProbNode[1]\n",
    "                    prediction.at[a,'Prediction@'+str(i+1)] = testingQueryPred\n",
    "                \n",
    "\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c277c8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8bb07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(prediction,k=10):\n",
    "    precision_list = pd.DataFrame(index=range(1,k+1),columns=['precision@k'])\n",
    "    precision = 0\n",
    "    actual = prediction['actual']\n",
    "    temp_prediction = prediction\n",
    "    for j in range(k):\n",
    "        precision = precision + len(temp_prediction[temp_prediction['Prediction@'+str(j+1)]==temp_prediction['actual']])\n",
    "        precision_list.at[j+1,'precision@k'] = precision\n",
    "        temp = temp_prediction[temp_prediction['Prediction@'+str(j+1)]!=temp_prediction['actual']]\n",
    "        temp_prediction = temp\n",
    "    return precision_list\n",
    "\n",
    "\n",
    "\n",
    "def Recall(prediction,method,k=[2,6]):\n",
    "    recall = pd.DataFrame(index= range(2*len(prediction.actual.unique())),columns = ['p','TP',\"TP+FN\"])\n",
    "    i = 0\n",
    "    for a in range(len(k)):\n",
    "        pred = prediction[['Prediction@'+str(j+1) for j in range(k[a])]]\n",
    "        pred['actual'] = prediction['actual']\n",
    "        for p in prediction.actual.unique():\n",
    "            TP = 0\n",
    "            for col in ['Prediction@'+str(j+1) for j in range(k[a])]:    \n",
    "                temp = pred[(pred[col]==p)&(pred['actual']==p)]\n",
    "                index = temp.index\n",
    "                TP = TP + len(temp)\n",
    "                pred = pred.drop(index)\n",
    "\n",
    "\n",
    "\n",
    "            recall.at[i,'k'] = k[a]\n",
    "            recall.at[i,'p'] = p\n",
    "            recall.at[i,'TP'] = TP\n",
    "            recall.at[i,'TP+FN'] = len(prediction[prediction['actual']==p])\n",
    "            i = i+1\n",
    "    recall['recall'] = recall['TP']/recall['TP+FN']\n",
    "    recall['method'] = [method for i in range(len(recall))]\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f8007",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4231c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_plot(precision_list,m):\n",
    "    linewidth = 2\n",
    "    if m=='Method 1':\n",
    "        marker = 'o'\n",
    "        color = 'red'\n",
    "        linestyle = 'solid'\n",
    "        return plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "\n",
    "    elif m == 'Method 2':\n",
    "        marker = 'o'\n",
    "        color = 'red'\n",
    "        linestyle = 'dotted'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    \n",
    "    elif m == 'Baseline 1':\n",
    "        marker = '^'\n",
    "        color = 'green'\n",
    "        linestyle = '--'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    \n",
    "    elif m == 'Baseline 2':\n",
    "        marker = 'D'\n",
    "        color = 'black'\n",
    "        linestyle = 'dotted'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "\n",
    "\n",
    "    elif m == 'Baseline 5':\n",
    "        marker = '*'\n",
    "        color = 'brown'\n",
    "        linestyle = 'dotted'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    \n",
    "    elif m == 'Baseline 3':\n",
    "        marker = 'h'\n",
    "        color = 'orange'\n",
    "        linestyle = '--'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    \n",
    "    elif m == 'Baseline 4a':\n",
    "        marker = 'D'\n",
    "        color = 'purple'\n",
    "        linestyle = '-.'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    \n",
    "    elif m == 'Baseline 4b':\n",
    "        marker = 'x'\n",
    "        color = 'purple'\n",
    "        linestyle = '-.'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "   \n",
    "    elif m == 'Baseline 6':\n",
    "        marker = '8'\n",
    "        color = 'blue'\n",
    "        linestyle = ':'\n",
    "        plt.plot(precision_list.index,precision_list['precision@k']/222,marker=marker,color=color,linestyle=linestyle,linewidth = linewidth,label=m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414de10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
